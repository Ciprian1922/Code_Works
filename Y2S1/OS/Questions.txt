●Suppose a friend told you: “A system with one CPU (with 4 cores) is better than one with 4 CPUs (each CPU having 1 single core) in an OS designed for autonomous vehicles.” Explain whether your friend is correct.
-Depends on the workload and the optimization of the operating system. In scenarios where tasks can be effectively parallelized, a single CPU with 4 cores might be better due to improved resource utilization. However, if the tasks are inherently sequential or the os is not optimized for parallel processing, having 4 separate CPUs might offer advantages in terms of individual task isolation and responsiveness.
●Give an example of a situation when a low-priority process could unblock the execution of a higher-priority process.
-Consider a scenario where a high-priority process is waiting for a particular resource held by a low-priority process. If the low-priority process voluntarily releases the resource or completes its task, it can unblock the execution of the higher-priority process, allowing it to proceed.
●Suppose a friend told you: “Streaming on a platform like Twitch/Facebook uses more I/O resources and less CPU than playing back a video on YouTube“. Explain if your friend is right, wrong or both.
-True, streaming on platforms like twitch or facebook generally involves more input/output resources, such as network bandwidth for real-time data transmission. In contrast, playing back a video on yt is more CPU-intensive, as the decoding and rendering of the video content rely heavily on the processor. So, streaming tends to prioritize I/O resources, while playing back a video on youtube emphasizes CPU usage. Also, the streamer usually does both streaming and watching his own stream while doing so, in order to check if it works as intended.
●Preemptive scheduling algorithms were designed for interactive systems with at least 2 cores per CPU.
-Not really, because preemptive scheduling algorithms are designed to handle multitasking in systems with single or multiple cores per CPU, so the use of preemptive scheduling is not dependent on the number of cores per CPU, it is implemented to allow the os to interrupt and switch between tasks, ensuring fairness and responsiveness in both single-core and multi-core systems.
●Calling yield() in a process’s primary thread causes the rest of the child threads to yield execution as well.
-Not quite, most threading models, including those commonly used in programming languages like java or python, calling yield() or a similar method in a process's primary thread only affects that specific thread, without making the rest of the child threads to yield execution. Each thread operates independently, and if you want to coordinate the yielding of multiple threads, explicit sync method need to be used.
●Forking a process in Linux determines the creation of a cloned process that starts to execute after parent process.
-Almost true, because when forking a process in Linux, a cloned process is is created, but the child process starts execution from the same point in the code as the parent process, not necessarily after the parent process. The parent and child processes continue their execution independently, and the order of their execution is not predetermined whatsoever, same as the behavior depends on the scheduling by the operating system.
●Page faults happening after forking a process in Linux will apply to both the parent and the child processes.
-Depends, when a process is forked in linux, the virtual memory space of the parent process is duplicated for the child process. Initially, the pages are marked as copy-on-write, meaning they are shared between the parent and the child, but if either process modifies a page, a page fault occurs, and the operating system creates a separate copy of that page for the modifying process. Therefore, page faults happening after forking may lead to separate copies of pages for both the parent and the child processes.
●In a mission-critical environment, it’s better to have parallel threads, rather than multiple processes.
-In a mission-critical environment, it's generally better to have parallel threads than multiple processes, because threads share the same memory space, allowing for more efficient communication and synchronization, which is crucial in mission-critical scenarios.
●After forking a parent process in linux after a critical region is created, the child process will still be able to access the critical region.
-After forking a parent process in linux, both the parent and the child processes will have their separate copies of the critical region, so changes made by one process in the critical region does not affect the other, as each process has its own memory space.
●Non-preemptive scheduling algorithms cannot schedule 3 processes on a CPU with one single core.
-False, non-preemptive scheduling algorithms can schedule multiple processes on a CPU with one single core, because the scheduler allows a process to run until it voluntarily gives up the CPU, completes its execution, or enters a waiting state.
●A process can voluntarily bypass pre-emption in interactive operating systems, to allow other processes to run.
-Yes, in interactive operating systems a process can voluntarily yield or give up the CPU to allow other processes to run, therefore bypassing pre-emption.
●Threads in a process could be scheduled by the operating system independently of processes themselves.
-Yes, threads in a process can be scheduled independently by the operating system, because they share the same memory space, allowing the operating system to schedule them independently without the need for complex context switching and memory management, as required for separate processes.
●Page faults are the reason why an interactive operating system is slowing down permanently over time.
-Not quite, page faults are not the only reason for permanent slowdown in an interactive operating system. Page faults can cause temporary slowdowns due to memory access delays, but other factors like inefficient algorithms, resource conflict, or hardware limitations can contribute to permanent slowdowns.
●Page faults happen more often on mobile operating systems with a lot of physical memory.
-Page faults occur when a program accesses data not currently in RAM, and they can happen on any system, with any amount of physical memory, but usually, even if the phone has a lot of memory, it is still likely to have less memory than a modern computer this days, so it might have more page faults when accessing data.(S23 Ultra with 12GB RAM and 512GB memory vs 32RAM pc and 1TB ssd, clearly the phone it is more likely to have more page faults. )
●On multi-processor systems, it’s required to protect data reads to memory with critical regions to avoid data corruption.
-No, on multi-processor systems, protecting data reads with critical regions is generally not necessary, as modern processors often have mechanisms, such as cache consistency protocols, to ensure data consistency between processors without the need for explicit critical regions.
●If a process has two threads, one with the lowest priority, and the other with the highest priority, what is the priority of the process itself?
-The priority of the process is determined by the highest priority thread within it, but it really depends on the scheduling procedure and how thread priorities are considered in the overall process priority. In some systems, the process priority might be an aggregate or a function of its thread priorities.
●Preemptive scheduling algorithms cannot schedule 5 processes on a system with two CPUs, each with one single core.
-False, preemptive scheduling algorithms can schedule multiple processes on a system with two CPUs, even if each CPU has a single core, because the number of processes that can be scheduled is not always determined by the number of cores but also depends on the scheduling algorithm and system configuration.
●An interactive operating system schedules threads dealing with UI/UX (user interface/ user experience) more often than those dealing with CPU-intensive operations.
-The frequency of scheduling threads in an interactive operating system depends on the specific workload, because the threads handling UI/UX may be scheduled more often if user responsiveness is a priority, but the scheduling behavior ultimately relies on the OS design and workload characteristics.
●Data corruption due to unprotected reads is the main reason why an interactive operating system is slowing down over time.
-Unprotected reads that lead to data corruption are one potential reason for slowdowns in an interactive operating system, but also inefficient algorithms, resource conflicts, or hardware limitations can contribute to permanent slowdowns.
●On multi-core processors, it's required to protect data reads from the processor's cache memory with critical regions to avoid data corruption.
-Yes, it is is necessary to do that in order to prevent data corruption, due to potential parallel access and inconsistent states.
●If a process has three threads having different priorities scheduled through SRTN, what is the priority of the process itself assuming that the normal priority thread finishes lastly?
-The priority of the process itself would be determined by the priority of the thread with the highest priority that is still active, so in this case, since the normal priority thread finishes last, the process's priority is high.
